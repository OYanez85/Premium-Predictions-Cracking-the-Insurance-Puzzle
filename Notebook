{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":84896,"databundleVersionId":10305135,"sourceType":"competition"}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 0. Introduction","metadata":{}},{"cell_type":"markdown","source":"This project is an example of Supervised Machine Learning, Regression Task.\n\n## Overview of Supervised Learning\n\nThe model is trained on labeled data (features and their corresponding Premium Amount).\nThe goal is to learn a mapping between the input features and the target variable.\n\n## Objective: Regression Task\n\nThe target variable (Premium Amount) is continuous, so the task is to predict a continuous value rather than a category.\n\n- Why Regression?\n\nThe objective is to predict a numeric value (insurance premium). Hence, this problem falls under regression rather than classification.\n\n- Key Takeaways\n\n    The pipeline demonstrates a typical workflow in supervised regression, including preprocessing, feature engineering, model training, and evaluation.\n    Using an ensemble model like Random Forest ensures robustness and handles non-linear relationships effectively.\nMetrics like RMSLE are chosen to suit the nature of the target variable and competition requirements.\n    This approach can be adapted to similar regression tasks involving tabular datasets.\n\n## Why Itâ€™s Multiple Linear Regression\nFeatures:\n\nThis project uses multiple input features such as Age, Annual Income, Health Score, Number of Dependents, and others. This clearly indicates a multiple variable setup.\n\n- Regression Task:\n\n    The goal is to predict a continuous target (Premium Amount), which is a regression task.\n\n- Model:\n\n    While Random Forest is a tree-based ensemble model and not explicitly a \"linear\" model, the problem setup itself fits the multiple regression context because it involves multiple input variables predicting one continuous target.","metadata":{}},{"cell_type":"markdown","source":"# 1. Import Required Libraries","metadata":{}},{"cell_type":"code","source":"# Import libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import mean_squared_log_error\nfrom xgboost import XGBRegressor\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T04:42:51.590388Z","iopub.execute_input":"2024-12-09T04:42:51.590857Z","iopub.status.idle":"2024-12-09T04:42:53.391660Z","shell.execute_reply.started":"2024-12-09T04:42:51.590790Z","shell.execute_reply":"2024-12-09T04:42:53.390534Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# 2. Load and Inspect the Data","metadata":{}},{"cell_type":"code","source":"# Load datasets\ntrain = pd.read_csv('/kaggle/input/playground-series-s4e12/train.csv')\ntest = pd.read_csv('/kaggle/input/playground-series-s4e12/test.csv')\n\n# Check for missing values and data types\nprint(\"Train Data Info:\")\nprint(train.info())\nprint(\"\\nTest Data Info:\")\nprint(test.info())\n\n# Extract target variable\ny = train['Premium Amount']\ntrain.drop(columns=['Premium Amount'], inplace=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T04:42:53.393638Z","iopub.execute_input":"2024-12-09T04:42:53.394226Z","iopub.status.idle":"2024-12-09T04:43:05.547876Z","shell.execute_reply.started":"2024-12-09T04:42:53.394190Z","shell.execute_reply":"2024-12-09T04:43:05.546638Z"}},"outputs":[{"name":"stdout","text":"Train Data Info:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1200000 entries, 0 to 1199999\nData columns (total 21 columns):\n #   Column                Non-Null Count    Dtype  \n---  ------                --------------    -----  \n 0   id                    1200000 non-null  int64  \n 1   Age                   1181295 non-null  float64\n 2   Gender                1200000 non-null  object \n 3   Annual Income         1155051 non-null  float64\n 4   Marital Status        1181471 non-null  object \n 5   Number of Dependents  1090328 non-null  float64\n 6   Education Level       1200000 non-null  object \n 7   Occupation            841925 non-null   object \n 8   Health Score          1125924 non-null  float64\n 9   Location              1200000 non-null  object \n 10  Policy Type           1200000 non-null  object \n 11  Previous Claims       835971 non-null   float64\n 12  Vehicle Age           1199994 non-null  float64\n 13  Credit Score          1062118 non-null  float64\n 14  Insurance Duration    1199999 non-null  float64\n 15  Policy Start Date     1200000 non-null  object \n 16  Customer Feedback     1122176 non-null  object \n 17  Smoking Status        1200000 non-null  object \n 18  Exercise Frequency    1200000 non-null  object \n 19  Property Type         1200000 non-null  object \n 20  Premium Amount        1200000 non-null  float64\ndtypes: float64(9), int64(1), object(11)\nmemory usage: 192.3+ MB\nNone\n\nTest Data Info:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 800000 entries, 0 to 799999\nData columns (total 20 columns):\n #   Column                Non-Null Count   Dtype  \n---  ------                --------------   -----  \n 0   id                    800000 non-null  int64  \n 1   Age                   787511 non-null  float64\n 2   Gender                800000 non-null  object \n 3   Annual Income         770140 non-null  float64\n 4   Marital Status        787664 non-null  object \n 5   Number of Dependents  726870 non-null  float64\n 6   Education Level       800000 non-null  object \n 7   Occupation            560875 non-null  object \n 8   Health Score          750551 non-null  float64\n 9   Location              800000 non-null  object \n 10  Policy Type           800000 non-null  object \n 11  Previous Claims       557198 non-null  float64\n 12  Vehicle Age           799997 non-null  float64\n 13  Credit Score          708549 non-null  float64\n 14  Insurance Duration    799998 non-null  float64\n 15  Policy Start Date     800000 non-null  object \n 16  Customer Feedback     747724 non-null  object \n 17  Smoking Status        800000 non-null  object \n 18  Exercise Frequency    800000 non-null  object \n 19  Property Type         800000 non-null  object \ndtypes: float64(8), int64(1), object(11)\nmemory usage: 122.1+ MB\nNone\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"print(train.columns)  # Confirm that 'Premium Amount' is not in train\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T04:43:05.549051Z","iopub.execute_input":"2024-12-09T04:43:05.549427Z","iopub.status.idle":"2024-12-09T04:43:05.555124Z","shell.execute_reply.started":"2024-12-09T04:43:05.549392Z","shell.execute_reply":"2024-12-09T04:43:05.553989Z"}},"outputs":[{"name":"stdout","text":"Index(['id', 'Age', 'Gender', 'Annual Income', 'Marital Status',\n       'Number of Dependents', 'Education Level', 'Occupation', 'Health Score',\n       'Location', 'Policy Type', 'Previous Claims', 'Vehicle Age',\n       'Credit Score', 'Insurance Duration', 'Policy Start Date',\n       'Customer Feedback', 'Smoking Status', 'Exercise Frequency',\n       'Property Type'],\n      dtype='object')\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# If you want to use X (features) and y (target)\nX = train  # 'train' now contains only the features\nprint(X.head())\nprint(y.head())  # 'y' contains the target variable\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T04:43:05.557358Z","iopub.execute_input":"2024-12-09T04:43:05.557709Z","iopub.status.idle":"2024-12-09T04:43:05.587391Z","shell.execute_reply.started":"2024-12-09T04:43:05.557677Z","shell.execute_reply":"2024-12-09T04:43:05.586175Z"}},"outputs":[{"name":"stdout","text":"   id   Age  Gender  Annual Income Marital Status  Number of Dependents  \\\n0   0  19.0  Female        10049.0        Married                   1.0   \n1   1  39.0  Female        31678.0       Divorced                   3.0   \n2   2  23.0    Male        25602.0       Divorced                   3.0   \n3   3  21.0    Male       141855.0        Married                   2.0   \n4   4  21.0    Male        39651.0         Single                   1.0   \n\n  Education Level     Occupation  Health Score  Location    Policy Type  \\\n0      Bachelor's  Self-Employed     22.598761     Urban        Premium   \n1        Master's            NaN     15.569731     Rural  Comprehensive   \n2     High School  Self-Employed     47.177549  Suburban        Premium   \n3      Bachelor's            NaN     10.938144     Rural          Basic   \n4      Bachelor's  Self-Employed     20.376094     Rural        Premium   \n\n   Previous Claims  Vehicle Age  Credit Score  Insurance Duration  \\\n0              2.0         17.0         372.0                 5.0   \n1              1.0         12.0         694.0                 2.0   \n2              1.0         14.0           NaN                 3.0   \n3              1.0          0.0         367.0                 1.0   \n4              0.0          8.0         598.0                 4.0   \n\n            Policy Start Date Customer Feedback Smoking Status  \\\n0  2023-12-23 15:21:39.134960              Poor             No   \n1  2023-06-12 15:21:39.111551           Average            Yes   \n2  2023-09-30 15:21:39.221386              Good            Yes   \n3  2024-06-12 15:21:39.226954              Poor            Yes   \n4  2021-12-01 15:21:39.252145              Poor            Yes   \n\n  Exercise Frequency Property Type  \n0             Weekly         House  \n1            Monthly         House  \n2             Weekly         House  \n3              Daily     Apartment  \n4             Weekly         House  \n0    2869.0\n1    1483.0\n2     567.0\n3     765.0\n4    2022.0\nName: Premium Amount, dtype: float64\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# 3. Preprocessing","metadata":{}},{"cell_type":"code","source":"# Handle missing values\n# Select numerical columns\nnum_cols = train.select_dtypes(include=['float64', 'int64']).columns\nif 'Premium Amount' in num_cols:\n    num_cols = num_cols.drop(['id', 'Premium Amount'])\nelse:\n    num_cols = num_cols.drop(['id'])\n\n# Select categorical columns\ncat_cols = train.select_dtypes(include=['object']).columns\n\n# Fill missing values for numerical columns\ntrain[num_cols] = train[num_cols].apply(lambda col: col.fillna(col.median()))\ntest[num_cols] = test[num_cols].apply(lambda col: col.fillna(col.median()))\n\n# Fill missing values for categorical columns\ntrain[cat_cols] = train[cat_cols].apply(lambda col: col.fillna('Unknown'))\ntest[cat_cols] = test[cat_cols].apply(lambda col: col.fillna('Unknown'))\n\n# Check and process 'Policy Start Date' column if it exists\nif 'Policy Start Date' in train.columns:\n    train['Policy Start Days'] = pd.to_datetime(train['Policy Start Date']).apply(lambda x: (x - pd.Timestamp(\"2000-01-01\")).days)\n    test['Policy Start Days'] = pd.to_datetime(test['Policy Start Date']).apply(lambda x: (x - pd.Timestamp(\"2000-01-01\")).days)\n\n    # Drop the original Policy Start Date column\n    train.drop(columns=['Policy Start Date'], inplace=True)\n    test.drop(columns=['Policy Start Date'], inplace=True)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T04:43:05.588710Z","iopub.execute_input":"2024-12-09T04:43:05.589046Z","iopub.status.idle":"2024-12-09T04:43:23.892989Z","shell.execute_reply.started":"2024-12-09T04:43:05.589013Z","shell.execute_reply":"2024-12-09T04:43:23.892125Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# 4. Train-Test Split for Cross-Validation","metadata":{}},{"cell_type":"code","source":"# Split the data into training and validation sets\nX_train, X_valid, y_train, y_valid = train_test_split(\n    train.drop(columns=['id']), y, test_size=0.2, random_state=42\n)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T04:43:23.894268Z","iopub.execute_input":"2024-12-09T04:43:23.894618Z","iopub.status.idle":"2024-12-09T04:43:25.069944Z","shell.execute_reply.started":"2024-12-09T04:43:23.894576Z","shell.execute_reply":"2024-12-09T04:43:25.068872Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# 5. Cross-Validation with XGBoost","metadata":{}},{"cell_type":"code","source":"X_test = test.copy()  # Initialize X_test from the test dataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T04:43:25.071277Z","iopub.execute_input":"2024-12-09T04:43:25.071716Z","iopub.status.idle":"2024-12-09T04:43:25.437972Z","shell.execute_reply.started":"2024-12-09T04:43:25.071670Z","shell.execute_reply":"2024-12-09T04:43:25.436974Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Retain the 'id' column for submission\nid_column = X_test['id']  # Save the 'id' column for later use\nX_test = X_test.drop(columns=['id'])  # Drop 'id' column for processing\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T04:43:25.439212Z","iopub.execute_input":"2024-12-09T04:43:25.439536Z","iopub.status.idle":"2024-12-09T04:43:25.547435Z","shell.execute_reply.started":"2024-12-09T04:43:25.439503Z","shell.execute_reply":"2024-12-09T04:43:25.546515Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Split training data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(\n    train, y, test_size=0.2, random_state=42\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T04:43:25.548702Z","iopub.execute_input":"2024-12-09T04:43:25.549080Z","iopub.status.idle":"2024-12-09T04:43:26.515231Z","shell.execute_reply.started":"2024-12-09T04:43:25.549027Z","shell.execute_reply":"2024-12-09T04:43:26.514144Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Define categorical columns\ncat_cols = X_train.select_dtypes(include=['object']).columns\n\n# Convert categorical columns to 'category' dtype\nX_train[cat_cols] = X_train[cat_cols].astype('category')\nX_val[cat_cols] = X_val[cat_cols].astype('category')\nX_test[cat_cols] = X_test[cat_cols].astype('category')\n\n# Ensure numerical columns are processed (e.g., missing values filled)\nnum_cols = X_train.select_dtypes(include=['float64', 'int64']).columns\n\n# Explicitly handle missing values for numerical columns\nfor col in num_cols:\n    if col in X_test.columns:  # Check if the column exists in X_test\n        X_train.loc[:, col] = X_train[col].fillna(X_train[col].median())\n        X_val.loc[:, col] = X_val[col].fillna(X_train[col].median())\n        X_test.loc[:, col] = X_test[col].fillna(X_train[col].median())\n    else:\n        # Handle cases where the column is missing in X_test\n        print(f\"Column '{col}' is missing in X_test and will be skipped.\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T04:43:26.518797Z","iopub.execute_input":"2024-12-09T04:43:26.519200Z","iopub.status.idle":"2024-12-09T04:43:28.886427Z","shell.execute_reply.started":"2024-12-09T04:43:26.519163Z","shell.execute_reply":"2024-12-09T04:43:28.885361Z"}},"outputs":[{"name":"stdout","text":"Column 'id' is missing in X_test and will be skipped.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Check for any remaining missing values\nprint(\"Missing values in X_train:\", X_train.isnull().sum().sum())\nprint(\"Missing values in X_val:\", X_val.isnull().sum().sum())\nprint(\"Missing values in X_test:\", X_test.isnull().sum().sum())\n\n# Verify data types\nprint(\"X_train dtypes:\\n\", X_train.dtypes)\nprint(\"X_val dtypes:\\n\", X_val.dtypes)\nprint(\"X_test dtypes:\\n\", X_test.dtypes)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T04:43:28.887809Z","iopub.execute_input":"2024-12-09T04:43:28.888142Z","iopub.status.idle":"2024-12-09T04:43:28.946250Z","shell.execute_reply.started":"2024-12-09T04:43:28.888106Z","shell.execute_reply":"2024-12-09T04:43:28.945138Z"}},"outputs":[{"name":"stdout","text":"Missing values in X_train: 0\nMissing values in X_val: 0\nMissing values in X_test: 0\nX_train dtypes:\n id                         int64\nAge                      float64\nGender                  category\nAnnual Income            float64\nMarital Status          category\nNumber of Dependents     float64\nEducation Level         category\nOccupation              category\nHealth Score             float64\nLocation                category\nPolicy Type             category\nPrevious Claims          float64\nVehicle Age              float64\nCredit Score             float64\nInsurance Duration       float64\nCustomer Feedback       category\nSmoking Status          category\nExercise Frequency      category\nProperty Type           category\nPolicy Start Days          int64\ndtype: object\nX_val dtypes:\n id                         int64\nAge                      float64\nGender                  category\nAnnual Income            float64\nMarital Status          category\nNumber of Dependents     float64\nEducation Level         category\nOccupation              category\nHealth Score             float64\nLocation                category\nPolicy Type             category\nPrevious Claims          float64\nVehicle Age              float64\nCredit Score             float64\nInsurance Duration       float64\nCustomer Feedback       category\nSmoking Status          category\nExercise Frequency      category\nProperty Type           category\nPolicy Start Days          int64\ndtype: object\nX_test dtypes:\n Age                      float64\nGender                  category\nAnnual Income            float64\nMarital Status          category\nNumber of Dependents     float64\nEducation Level         category\nOccupation              category\nHealth Score             float64\nLocation                category\nPolicy Type             category\nPrevious Claims          float64\nVehicle Age              float64\nCredit Score             float64\nInsurance Duration       float64\nCustomer Feedback       category\nSmoking Status          category\nExercise Frequency      category\nProperty Type           category\nPolicy Start Days          int64\ndtype: object\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"print(f\"X_train shape: {X_train.shape}\")\nprint(f\"X_val shape: {X_val.shape}\")\nprint(f\"X_test shape: {X_test.shape}\")\n\nprint(f\"Categorical columns: {cat_cols}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T04:43:28.947536Z","iopub.execute_input":"2024-12-09T04:43:28.947883Z","iopub.status.idle":"2024-12-09T04:43:28.953677Z","shell.execute_reply.started":"2024-12-09T04:43:28.947850Z","shell.execute_reply":"2024-12-09T04:43:28.952695Z"}},"outputs":[{"name":"stdout","text":"X_train shape: (960000, 20)\nX_val shape: (240000, 20)\nX_test shape: (800000, 19)\nCategorical columns: Index(['Gender', 'Marital Status', 'Education Level', 'Occupation', 'Location',\n       'Policy Type', 'Customer Feedback', 'Smoking Status',\n       'Exercise Frequency', 'Property Type'],\n      dtype='object')\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# 6: Model Training on Full Data and Cross-Validation","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_log_error\nimport numpy as np\n\n# Initialize the XGBoost Regressor\nxgb_model = XGBRegressor(\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=6,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    random_state=42,\n    enable_categorical=True  # For categorical data\n)\n\n# Train the model on X_train and validate on X_val\nxgb_model.fit(\n    X_train.drop(columns=['id']),  # Drop non-feature columns\n    y_train,\n    early_stopping_rounds=50,\n    eval_set=[(X_val.drop(columns=['id']), y_val)],\n    verbose=50\n)\n\n# Predict on validation set\nval_preds = xgb_model.predict(X_val.drop(columns=['id']))\n\n# Calculate RMSLE\nrmsle = np.sqrt(mean_squared_log_error(y_val, np.maximum(0, val_preds)))\nprint(f\"Validation RMSLE: {rmsle}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T04:43:28.955172Z","iopub.execute_input":"2024-12-09T04:43:28.956110Z","iopub.status.idle":"2024-12-09T04:44:27.753598Z","shell.execute_reply.started":"2024-12-09T04:43:28.956042Z","shell.execute_reply":"2024-12-09T04:44:27.752462Z"}},"outputs":[{"name":"stdout","text":"[0]\tvalidation_0-rmse:862.80931\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[50]\tvalidation_0-rmse:845.19575\n[100]\tvalidation_0-rmse:842.58514\n[150]\tvalidation_0-rmse:841.62786\n[200]\tvalidation_0-rmse:840.86132\n[250]\tvalidation_0-rmse:840.08339\n[300]\tvalidation_0-rmse:839.78725\n[350]\tvalidation_0-rmse:839.58713\n[400]\tvalidation_0-rmse:839.26234\n[450]\tvalidation_0-rmse:839.07970\n[499]\tvalidation_0-rmse:838.91257\nValidation RMSLE: 1.139874106038963\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"# 7: Predict on Test Data and Prepare Submission","metadata":{}},{"cell_type":"code","source":"# Ensure the 'id' column is preserved from the original test dataset\nid_column = test['id']  # Save the 'id' column separately\n\n# Predict on the test dataset\n# Drop 'id' only if it exists in X_test, to avoid KeyError\nif 'id' in X_test.columns:\n    X_test = X_test.drop(columns=['id'])\n\n# Make predictions using the model\ntest_preds = xgb_model.predict(X_test)\n\n# Create a submission DataFrame\nsubmission = pd.DataFrame({\n    'id': id_column,  # Use the preserved 'id' column\n    'Premium Amount': np.maximum(0, test_preds)  # Ensure no negative predictions\n})\n\n# Save the submission file\nsubmission.to_csv('submission.csv', index=False)\nprint(\"Submission file saved as 'submission.csv'\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T04:46:14.897221Z","iopub.execute_input":"2024-12-09T04:46:14.897659Z","iopub.status.idle":"2024-12-09T04:46:23.008436Z","shell.execute_reply.started":"2024-12-09T04:46:14.897620Z","shell.execute_reply":"2024-12-09T04:46:23.007339Z"}},"outputs":[{"name":"stdout","text":"Submission file saved as 'submission.csv'\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# 8. Cross-Validation with K-Fold","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold\n\n# Update the XGBoost model to include early stopping rounds in the constructor\nxgb_model = XGBRegressor(\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=6,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    random_state=42,\n    enable_categorical=True,  # For categorical data\n    early_stopping_rounds=50  # Add this to the constructor\n)\n\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\ncv_results = []\n\nfor train_index, val_index in kf.split(X_train):\n    X_tr, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n    y_tr, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n\n    # Train the model\n    xgb_model.fit(\n        X_tr.drop(columns=['id']),\n        y_tr,\n        eval_set=[(X_val_fold.drop(columns=['id']), y_val_fold)],\n        verbose=False\n    )\n\n    # Predict and calculate RMSLE\n    val_preds_fold = xgb_model.predict(X_val_fold.drop(columns=['id']))\n    fold_rmsle = np.sqrt(mean_squared_log_error(y_val_fold, np.maximum(0, val_preds_fold)))\n    cv_results.append(fold_rmsle)\n\nprint(f\"Cross-Validation RMSLE scores: {cv_results}\")\nprint(f\"Mean RMSLE: {np.mean(cv_results)}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T04:46:49.258395Z","iopub.execute_input":"2024-12-09T04:46:49.258823Z","iopub.status.idle":"2024-12-09T04:50:48.930672Z","shell.execute_reply.started":"2024-12-09T04:46:49.258785Z","shell.execute_reply":"2024-12-09T04:50:48.929568Z"}},"outputs":[{"name":"stdout","text":"Cross-Validation RMSLE scores: [1.1412591750529921, 1.1368383830179096, 1.1373502342492805, 1.139760262187529, 1.1364341915908904]\nMean RMSLE: 1.13832844921972\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"The cross-validation results show that your model is performing consistently across the folds, with RMSLE values slightly above 1.13. The mean RMSLE of 1.1383 indicates that the model has reasonable predictive power, though there is room for improvement. Here are some suggestions and next steps:","metadata":{}},{"cell_type":"markdown","source":"# 9. Recommendations for improvement","metadata":{}},{"cell_type":"markdown","source":"## 9.1. Hyperparameter tunning","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBRegressor\n\n# Check for negative target values and adjust if needed\nif (y_train < 0).any():\n    print(\"Warning: Negative target values detected. Adjusting to non-negative.\")\n    y_train = y_train.clip(lower=0)\n\n# Define parameter grid\nparam_grid = {\n    'max_depth': [4, 6, 8],\n    'learning_rate': [0.01, 0.05, 0.1],\n    'subsample': [0.6, 0.8, 1.0],\n    'colsample_bytree': [0.6, 0.8, 1.0],\n    'n_estimators': [100, 300, 500],\n}\n\n# Initialize GridSearchCV with a different scoring metric\ngrid_search = GridSearchCV(\n    estimator=XGBRegressor(random_state=42, enable_categorical=True),\n    param_grid=param_grid,\n    scoring='neg_mean_squared_error',  # Change to a compatible scoring metric\n    cv=3,\n    verbose=1\n)\n\n# Fit the grid search model\ngrid_search.fit(X_train.drop(columns=['id']), y_train)\n\n# Best parameters\nprint(f\"Best Parameters: {grid_search.best_params_}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T04:52:12.266147Z","iopub.execute_input":"2024-12-09T04:52:12.266555Z"}},"outputs":[{"name":"stdout","text":"Fitting 3 folds for each of 243 candidates, totalling 729 fits\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"## 9.2. Feature Engineering","metadata":{}},{"cell_type":"code","source":"columns_to_drop = ['Policy Start Days']  # Replace with your columns\nfor col in columns_to_drop:\n    if col in X_train.columns:\n        X_train.drop(columns=[col], inplace=True)\n    if col in X_val.columns:\n        X_val.drop(columns=[col], inplace=True)\n    if col in X_test.columns:\n        X_test.drop(columns=[col], inplace=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T04:44:29.256509Z","iopub.status.idle":"2024-12-09T04:44:29.257099Z","shell.execute_reply.started":"2024-12-09T04:44:29.256794Z","shell.execute_reply":"2024-12-09T04:44:29.256824Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"X_train columns:\", X_train.columns)\nprint(\"X_val columns:\", X_val.columns)\nprint(\"X_test columns:\", X_test.columns)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T04:44:29.259532Z","iopub.status.idle":"2024-12-09T04:44:29.260103Z","shell.execute_reply.started":"2024-12-09T04:44:29.259810Z","shell.execute_reply":"2024-12-09T04:44:29.259838Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example of column creation\nif 'Policy Start Date' in X_train.columns:\n    X_train['Policy Start Days'] = (pd.to_datetime(X_train['Policy Start Date']) - pd.Timestamp(\"2000-01-01\")).dt.days\n    X_val['Policy Start Days'] = (pd.to_datetime(X_val['Policy Start Date']) - pd.Timestamp(\"2000-01-01\")).dt.days\n    X_test['Policy Start Days'] = (pd.to_datetime(X_test['Policy Start Date']) - pd.Timestamp(\"2000-01-01\")).dt.days\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T04:44:29.261217Z","iopub.status.idle":"2024-12-09T04:44:29.261747Z","shell.execute_reply.started":"2024-12-09T04:44:29.261468Z","shell.execute_reply":"2024-12-09T04:44:29.261497Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"irrelevant_columns = [col for col in ['Policy Start Days'] if col in X_train.columns]\nX_train.drop(columns=irrelevant_columns, inplace=True)\nX_val.drop(columns=irrelevant_columns, inplace=True)\nX_test.drop(columns=irrelevant_columns, inplace=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T04:44:29.263786Z","iopub.status.idle":"2024-12-09T04:44:29.264326Z","shell.execute_reply.started":"2024-12-09T04:44:29.264038Z","shell.execute_reply":"2024-12-09T04:44:29.264085Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Feature engineering\nX_train['Age_Income_Interaction'] = X_train['Age'] * X_train['Annual Income']\nX_val['Age_Income_Interaction'] = X_val['Age'] * X_val['Annual Income']\nX_test['Age_Income_Interaction'] = X_test['Age'] * X_test['Annual Income']\n\n# Drop irrelevant columns dynamically\ncolumns_to_drop = ['Policy Start Days']\nfor col in columns_to_drop:\n    if col in X_train.columns:\n        X_train.drop(columns=[col], inplace=True)\n    if col in X_val.columns:\n        X_val.drop(columns=[col], inplace=True)\n    if col in X_test.columns:\n        X_test.drop(columns=[col], inplace=True)\n\nprint(\"Feature engineering and column dropping completed successfully.\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T04:44:29.265591Z","iopub.status.idle":"2024-12-09T04:44:29.266128Z","shell.execute_reply.started":"2024-12-09T04:44:29.265844Z","shell.execute_reply":"2024-12-09T04:44:29.265871Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 9.3. Model ensembling","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\n# Define the encoder for categorical columns\ncat_cols = X_train.select_dtypes(include=['category']).columns\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols)\n    ],\n    remainder='passthrough'  # Keep numeric columns as they are\n)\n\n# Transform the data\nX_train_encoded = preprocessor.fit_transform(X_train)\nX_val_encoded = preprocessor.transform(X_val)\nX_test_encoded = preprocessor.transform(X_test)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T04:44:29.267842Z","iopub.status.idle":"2024-12-09T04:44:29.268372Z","shell.execute_reply.started":"2024-12-09T04:44:29.268107Z","shell.execute_reply":"2024-12-09T04:44:29.268136Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\n# Identify categorical columns\ncat_cols = X_train.select_dtypes(include=['category']).columns\n\n# Create a ColumnTransformer for preprocessing\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)\n    ],\n    remainder='passthrough'  # Leave numerical columns as is\n)\n\n# Apply preprocessing\nX_train_enc = preprocessor.fit_transform(X_train.drop(columns=['id']))\nX_val_enc = preprocessor.transform(X_val.drop(columns=['id']))\nX_test_enc = preprocessor.transform(X_test.drop(columns=['id']))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T04:44:29.270119Z","iopub.status.idle":"2024-12-09T04:44:29.270630Z","shell.execute_reply.started":"2024-12-09T04:44:29.270362Z","shell.execute_reply":"2024-12-09T04:44:29.270389Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_leaves = 2**6  # For max_depth=6, set num_leaves=64\nlgbm_model = LGBMRegressor(\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=6,\n    num_leaves=num_leaves,\n    random_state=42\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T04:44:29.272428Z","iopub.status.idle":"2024-12-09T04:44:29.272776Z","shell.execute_reply.started":"2024-12-09T04:44:29.272606Z","shell.execute_reply":"2024-12-09T04:44:29.272623Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lgbm_model = LGBMRegressor(\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=6,\n    num_leaves=64,  # Based on max_depth\n    min_child_samples=20,  # Minimum samples in a leaf\n    feature_fraction=0.8,  # Randomly use 80% of features for each tree\n    bagging_fraction=0.8,  # Randomly use 80% of data for each tree\n    force_row_wise=True,  # Enable row-wise processing\n    random_state=42\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T04:44:29.274307Z","iopub.status.idle":"2024-12-09T04:44:29.274634Z","shell.execute_reply.started":"2024-12-09T04:44:29.274477Z","shell.execute_reply":"2024-12-09T04:44:29.274494Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_ensemble = VotingRegressor([\n    ('xgb', xgb_model),\n    ('lgbm', lgbm_model)\n])\n\nmodel_ensemble.fit(X_train_enc, y_train)\nensemble_preds = model_ensemble.predict(X_val_enc)\nensemble_rmsle = np.sqrt(mean_squared_log_error(y_val, np.maximum(0, ensemble_preds)))\nprint(f\"Ensemble RMSLE: {ensemble_rmsle}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T04:44:29.275570Z","iopub.status.idle":"2024-12-09T04:44:29.275921Z","shell.execute_reply.started":"2024-12-09T04:44:29.275745Z","shell.execute_reply":"2024-12-09T04:44:29.275764Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lgbm_model = LGBMRegressor(\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=6,\n    num_leaves=64,\n    feature_fraction=0.8,  # Use feature_fraction OR bagging_fraction\n    subsample=0.8,         # Set subsample explicitly to avoid conflicts\n    random_state=42\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T04:44:29.277372Z","iopub.status.idle":"2024-12-09T04:44:29.277892Z","shell.execute_reply.started":"2024-12-09T04:44:29.277628Z","shell.execute_reply":"2024-12-09T04:44:29.277656Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from lightgbm import plot_importance\nimport matplotlib.pyplot as plt\n\nlgbm_model.fit(X_train_enc, y_train)\nplt.figure(figsize=(10, 8))\nplot_importance(lgbm_model, max_num_features=20)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T04:44:29.279782Z","iopub.status.idle":"2024-12-09T04:44:29.280162Z","shell.execute_reply.started":"2024-12-09T04:44:29.279960Z","shell.execute_reply":"2024-12-09T04:44:29.279978Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lgbm_model = LGBMRegressor(\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=6,\n    num_leaves=64,\n    min_child_samples=50,  # Default is 20; increase for regularization\n    subsample=0.8,\n    random_state=42\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T04:44:29.281741Z","iopub.status.idle":"2024-12-09T04:44:29.282105Z","shell.execute_reply.started":"2024-12-09T04:44:29.281914Z","shell.execute_reply":"2024-12-09T04:44:29.281932Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'max_depth': [4, 6, 8],\n    'num_leaves': [31, 64, 128],\n    'min_child_samples': [20, 50, 100],\n    'feature_fraction': [0.6, 0.8, 1.0],\n    'subsample': [0.6, 0.8, 1.0]\n}\n\ngrid_search = GridSearchCV(\n    estimator=LGBMRegressor(random_state=42),\n    param_grid=param_grid,\n    scoring='neg_mean_squared_log_error',\n    cv=3,\n    verbose=1\n)\n\ngrid_search.fit(X_train_enc, y_train)\nprint(f\"Best Parameters: {grid_search.best_params_}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T04:44:29.283270Z","iopub.status.idle":"2024-12-09T04:44:29.283601Z","shell.execute_reply.started":"2024-12-09T04:44:29.283442Z","shell.execute_reply":"2024-12-09T04:44:29.283460Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check for constant or near-constant features\nlow_variance_cols = [col for col in X_train_enc if X_train_enc[col].nunique() < 2]\nprint(f\"Low variance columns: {low_variance_cols}\")\n\n# Drop low-variance columns\nX_train_enc.drop(columns=low_variance_cols, inplace=True)\nX_val_enc.drop(columns=low_variance_cols, inplace=True)\nX_test_enc.drop(columns=low_variance_cols, inplace=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T04:44:29.285073Z","iopub.status.idle":"2024-12-09T04:44:29.285681Z","shell.execute_reply.started":"2024-12-09T04:44:29.285492Z","shell.execute_reply":"2024-12-09T04:44:29.285513Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import VotingRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\n# Define base models\nxgb_model = XGBRegressor(n_estimators=500, learning_rate=0.05, max_depth=6, random_state=42)\nlgbm_model = LGBMRegressor(n_estimators=500, learning_rate=0.05, max_depth=6, random_state=42)\n\n# Ensemble model\nmodel_ensemble = VotingRegressor([\n    ('xgb', xgb_model),\n    ('lgbm', lgbm_model)\n])\n\n# Train the ensemble model\nmodel_ensemble.fit(X_train_enc, y_train)\n\n# Predict and calculate RMSLE on the validation set\nensemble_preds = model_ensemble.predict(X_val_enc)\nensemble_rmsle = np.sqrt(mean_squared_log_error(y_val, np.maximum(0, ensemble_preds)))\nprint(f\"Ensemble RMSLE: {ensemble_rmsle}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T04:44:29.286643Z","iopub.status.idle":"2024-12-09T04:44:29.286991Z","shell.execute_reply.started":"2024-12-09T04:44:29.286817Z","shell.execute_reply":"2024-12-09T04:44:29.286836Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"X_train_enc shape: {X_train_enc.shape}\")\nprint(f\"X_val_enc shape: {X_val_enc.shape}\")\nprint(f\"X_test_enc shape: {X_test_enc.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T04:44:29.287816Z","iopub.status.idle":"2024-12-09T04:44:29.288173Z","shell.execute_reply.started":"2024-12-09T04:44:29.287977Z","shell.execute_reply":"2024-12-09T04:44:29.287994Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MyCustomError(Exception):\n    pass\nraise MyCustomError(\"The Jedi force stops you here\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T04:44:29.289973Z","iopub.status.idle":"2024-12-09T04:44:29.290362Z","shell.execute_reply.started":"2024-12-09T04:44:29.290170Z","shell.execute_reply":"2024-12-09T04:44:29.290189Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import VotingRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\n# Define models\nxgb_model = XGBRegressor(n_estimators=500, learning_rate=0.05, max_depth=6, random_state=42)\nlgbm_model = LGBMRegressor(n_estimators=500, learning_rate=0.05, max_depth=6, random_state=42)\n\n# Ensemble model\nmodel_ensemble = VotingRegressor([\n    ('xgb', xgb_model),\n    ('lgbm', lgbm_model)\n])\n\n# Fit the ensemble model\nmodel_ensemble.fit(X_train_enc, y_train)\n\n# Predict and calculate RMSLE on validation set\nensemble_preds = model_ensemble.predict(X_val_enc)\nensemble_rmsle = np.sqrt(mean_squared_log_error(y_val, np.maximum(0, ensemble_preds)))\nprint(f\"Ensemble RMSLE: {ensemble_rmsle}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T04:44:29.291804Z","iopub.status.idle":"2024-12-09T04:44:29.292176Z","shell.execute_reply.started":"2024-12-09T04:44:29.291976Z","shell.execute_reply":"2024-12-09T04:44:29.291993Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import VotingRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\n# Define models with optimized parameters\nxgb_model = XGBRegressor(\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=6,\n    random_state=42\n)\n\nlgbm_model = LGBMRegressor(\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=6,\n    num_leaves=63,  # 2^6 - 1 to align with max_depth\n    random_state=42\n)\n\n# Ensemble model\nmodel_ensemble = VotingRegressor([\n    ('xgb', xgb_model),\n    ('lgbm', lgbm_model)\n])\n\n# Fit the ensemble model\nmodel_ensemble.fit(X_train.drop(columns=['id']), y_train)\n\n# Predict and calculate RMSLE on validation set\nensemble_preds = model_ensemble.predict(X_val.drop(columns=['id']))\nensemble_rmsle = np.sqrt(mean_squared_log_error(y_val, np.maximum(0, ensemble_preds)))\nprint(f\"Ensemble RMSLE: {ensemble_rmsle}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T04:44:29.293588Z","iopub.status.idle":"2024-12-09T04:44:29.293962Z","shell.execute_reply.started":"2024-12-09T04:44:29.293781Z","shell.execute_reply":"2024-12-09T04:44:29.293801Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 9.4. Statistical aspects of multiple linear regression and the worth of polynomial regression with synthetic data. Residual Analysis. ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.graphics.gofplots import qqplot\nimport matplotlib.pyplot as plt\n\n# Split data\nX = train.drop(columns=['Premium Amount', 'id'])\ny = train['Premium Amount']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Add constant for intercept\nX_train_const = sm.add_constant(X_train)\nX_test_const = sm.add_constant(X_test)\n\n# Fit model\nmodel = sm.OLS(y_train, X_train_const).fit()\nprint(model.summary())\n\n# Predictions and metrics\ny_pred = model.predict(X_test_const)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nr2 = r2_score(y_test, y_pred)\n\nprint(f\"RMSE: {rmse}\")\nprint(f\"RÂ²: {r2}\")\n\n# Residual analysis\nresiduals = y_test - y_pred\nstandardized_residuals = (residuals - np.mean(residuals)) / np.std(residuals)\n\n# Q-Q plot for residual normality\nqqplot(standardized_residuals, line='s')\nplt.title('Q-Q Plot of Standardized Residuals')\nplt.show()\n\n# Heteroskedasticity test\nfrom statsmodels.stats.diagnostic import het_breuschpagan\nbp_test = het_breuschpagan(residuals, X_test_const)\nprint(f\"Breusch-Pagan test: {bp_test}\")\n\n# VIF for multicollinearity\nvif_data = pd.DataFrame()\nvif_data[\"feature\"] = X_train.columns\nvif_data[\"VIF\"] = [variance_inflation_factor(X_train_const.values, i + 1) for i in range(X_train.shape[1])]\n\nprint(\"VIF Data:\")\nprint(vif_data)\n\n# Cook's distance for influential values\ninfluence = model.get_influence()\n(c, p) = influence.cooks_distance\nplt.stem(np.arange(len(c)), c, markerfmt=\",\", use_line_collection=True)\nplt.title(\"Cook's Distance\")\nplt.show()\n\n# Residual plots\nplt.scatter(y_pred, residuals)\nplt.axhline(0, color='red', linestyle='--')\nplt.xlabel('Predicted Values')\nplt.ylabel('Residuals')\nplt.title('Residuals vs Predicted')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T04:44:29.296150Z","iopub.status.idle":"2024-12-09T04:44:29.296506Z","shell.execute_reply.started":"2024-12-09T04:44:29.296336Z","shell.execute_reply":"2024-12-09T04:44:29.296356Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Predict on validation set\nval_preds = xgb_model.predict(X_val.drop(columns=['id']))\n\n# Plot actual vs predicted values\nplt.scatter(y_val, val_preds, alpha=0.3)\nplt.xlabel('Actual Premium Amount')\nplt.ylabel('Predicted Premium Amount')\nplt.title('Actual vs. Predicted')\nplt.show()\n\n# Residual plot\nresiduals = y_val - val_preds\nplt.hist(residuals, bins=50, alpha=0.7)\nplt.xlabel('Residuals')\nplt.ylabel('Frequency')\nplt.title('Residual Distribution')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T04:44:29.297918Z","iopub.status.idle":"2024-12-09T04:44:29.298405Z","shell.execute_reply.started":"2024-12-09T04:44:29.298186Z","shell.execute_reply":"2024-12-09T04:44:29.298217Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 9.5. Test Polynomial Regression:","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\n\npoly = PolynomialFeatures(degree=2)\nX_poly = poly.fit_transform(X_train)\npoly_model = sm.OLS(y_train, sm.add_constant(X_poly)).fit()\nprint(poly_model.summary())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T04:44:29.299602Z","iopub.status.idle":"2024-12-09T04:44:29.299986Z","shell.execute_reply.started":"2024-12-09T04:44:29.299801Z","shell.execute_reply":"2024-12-09T04:44:29.299821Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MyCustomError(Exception):\n    pass\nraise MyCustomError(\"The Jedi force stops you here\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T04:44:29.301024Z","iopub.status.idle":"2024-12-09T04:44:29.301413Z","shell.execute_reply.started":"2024-12-09T04:44:29.301246Z","shell.execute_reply":"2024-12-09T04:44:29.301265Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 10. Submit the File to Kaggle","metadata":{}},{"cell_type":"code","source":"# Ensure 'id' is added back during submission\nprint(f\"Saving 'id' column for submission: {id_column.head()}\")\n\n# After training the model, use the saved 'id' column for submission\npredictions = xgb_model.predict(X_test)  # Replace 'model' with your trained model\nsubmission = pd.DataFrame({'id': id_column, 'Premium Amount': predictions})\nsubmission.to_csv('submission.csv', index=False)\n\nprint(\"Submission file created: submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T04:44:29.302852Z","iopub.status.idle":"2024-12-09T04:44:29.303258Z","shell.execute_reply.started":"2024-12-09T04:44:29.303041Z","shell.execute_reply":"2024-12-09T04:44:29.303082Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Predict on test set\ntest_preds = xgb_model.predict(X_test.drop(columns=['id']))\n\n# Create the submission DataFrame\nsubmission = pd.DataFrame({\n    'id': X_test['id'],\n    'Premium Amount': np.maximum(0, test_preds)  # Ensure non-negative predictions\n})\n\n# Save submission file\nsubmission.to_csv('submission.csv', index=False)\nprint(\"Submission file created: submission.csv\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T04:44:29.304530Z","iopub.status.idle":"2024-12-09T04:44:29.304907Z","shell.execute_reply.started":"2024-12-09T04:44:29.304732Z","shell.execute_reply":"2024-12-09T04:44:29.304752Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"kaggle competitions submit -c playground-series-s4e12 -f submission.csv -m \"XGBoost model with tuned parameters and cross-validation\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T04:44:29.306232Z","iopub.status.idle":"2024-12-09T04:44:29.306623Z","shell.execute_reply.started":"2024-12-09T04:44:29.306434Z","shell.execute_reply":"2024-12-09T04:44:29.306454Z"}},"outputs":[],"execution_count":null}]}